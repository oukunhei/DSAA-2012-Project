# DSAA 2012 Project

A reinforcement learning-based selector project for NL2SQL tasks, optimizing SQL candidate selection through SFT and DPO training.

## Project Overview

This project addresses the challenge of selecting the most appropriate SQL query from multiple candidates in NL2SQL tasks. By training an intelligent selector, we aim to choose the most suitable query statement from various SQL candidates. The project employs a two-stage training strategy: Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).

### Core Features

- **Intelligent Selector**: Efficient SQL selector trained on Qwen-3B model
- **Dual-stage Training**: SFT + DPO reinforcement learning training pipeline
- **Dual-scoring System**: Weighted evaluation combining clustering scores and selector scores
- **Multi-dataset Support**: Support for mainstream NL2SQL datasets like BIRD
- **Efficient Inference**: Support for batch processing and real-time inference

## Project Structure

```
nl2sql-selector/
├── data/                             # Data Directory
│   ├── bird/                        # BIRD Dataset
│   └── other_datasets/              # Other Datasets
│   ├── train_set/                   # Training Data
│   │   ├── train_sft/              # SFT Training Data
│   │   ├── train_dpo/              # DPO Training Data
│   │   ├── xiyan_candidates/       # SQL Candidate Data
│   └── test_set/                   # Test Data
│       ├── omin_eval.json          # Omin Evaluation Set
│       ├── deepeye_eval.json       # DeepEye Evaluation Set
│       └── qwen_eval.json          # Qwen Evaluation Set
├── src/                             # Source Code
│   ├── data_processing/            # Data Processing Module
│   │   ├── __init__.py
│   │   ├── dpo_train_set_generator.py    # DPO Training Set Generator
│   │   ├── sft_train_set_generator.py    # SFT Training Set Generator
│   │   └── cluster_processor.py    # Clustering Processor
│   ├── training/                   # Training Module
│   │   ├── __init__.py
│   │   └── trainer.py              # Full Pipeline Trainer
│   ├── test/                       # Testing & Inference Module
│   │   ├── __init__.py
│   │   ├── selector.py             # Selector Inference
│   │   ├── SC.py                   # SC Baseline Method
│   │   └── evaluation.py           # Evaluation & Analysis
├── outputs/                        # Output Directory
│   ├── models/                     # Trained Models
│   │   ├── sft_model/             # SFT Stage Model
│   │   └── dpo_model/             # DPO Stage Model
├── requirements.txt               # Dependencies List
├── main.py                       # Main Entry Point
└── README.md                     # Project Documentation
```

## Installation

```bash
pip install -r requirements.txt
```

Core Dependencies:
- torch >= 2.0.0
- transformers >= 4.30.0
- peft >= 0.4.0
- datasets >= 2.12.0
- sqlparse >= 0.4.4
- scikit-learn >= 1.2.0
- numpy >= 1.21.0
- pandas >= 1.5.0

## Quick Start

### 1. Data Preparation

Download the BIRD development dataset:

- Visit https://bird-bench.github.io/
- Download dev.zip from the website
- Save it to the data/ directory

### 2. Model Training

```bash
# SFT supervised fine-tuning
python main.py --mode train_sft --model_path Qwen/Qwen2.5-3B

# DPO reinforcement learning training
python main.py --mode train_dpo --sft_model_path outputs/models/sft_model
```

### 3. Model Evaluation

```bash
# Evaluate using selector
python selector.py

# Get self-consistency baseline
python SC.py

# Evaluate the differnece between the two methods
python evaluation.py

```

## Dataset Description

### Training Data
- **SFT Data**: Contains (schema, question, candidates, correct_sql) quadruples
- **DPO Data**: Contains (schema, question, candidates, chosen_sql, rejected_sql) quintuples

### Test Data
- **omin_eval.json**: Evaluation set with candidate SQLs generated by Omin model
- **deepeye_eval.json**: Evaluation set with candidate SQLs generated by DeepEye model  
- **qwen_eval.json**: Evaluation set with candidate SQLs generated by Qwen model

## Evaluation Metrics

- **Execution Accuracy**: Matching degree between SQL execution results and ground truth
- **Syntax Accuracy**: SQL syntax validation pass rate
- **Selection Accuracy**: Proportion of correct SQL selections by the selector
- **Relative Improvement**: Performance improvement compared to SC baseline

## Experimental Results

| SQL Gneration Model | Selection Strategy | Dev Excution Accuracy |
|--------|-------------------|-----------------|-------------------|
|   Qwen2.5-Coder-7b-Instruct | SC Baseline | 000% |
| XiyanSQL-7b | Selector | 000% | 

## Configuration

### Training Parameters
- Base Model: Qwen2.5-3B
- LoRA Rank: 16
- Learning Rate: 2e-4 (SFT), 1e-5 (DPO)
- Batch Size: 8
- Epochs: 3 (SFT), 2 (DPO)

### Inference Parameters
- Temperature: 0.7
- Max Input Length: 2048
- Max Output Length: 512

## Acknowledgments

- Thanks to the Qwen team for providing the base model
- Thanks to the BIRD dataset providers
- Thanks to all contributors and users
