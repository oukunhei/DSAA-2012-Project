# DSAA 2012 Project

A reinforcement learning-based selector project for NL2SQL tasks, optimizing SQL candidate selection through SFT and DPO training.

## Project Overview

This project addresses the challenge of selecting the most appropriate SQL query from multiple candidates in NL2SQL tasks. By training an intelligent selector, we aim to choose the most suitable query statement from various SQL candidates. The project employs a two-stage training strategy: Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).

### Core Features

- **Intelligent Selector**: Efficient SQL selector trained on Qwen-3B model
- **Dual-stage Training**: SFT + DPO reinforcement learning training pipeline
- **Dual-scoring System**: Weighted evaluation combining clustering scores and selector scores
- **Multi-dataset Support**: Support for mainstream NL2SQL generation strategy including Deepeye, Omni and Qwen

## Project Structure

```
nl2sql-selector/
├── data/                              # Data Directory
│   ├── bird/                          # BIRD Dataset
│   ├── train_set/                     # Training Data
│   │   ├── train_sft/                 # SFT Training Data
│   │   ├── train_dpo/                 # DPO Training Data
│   │   ├── xiyan_candidates/          # SQL Candidate Data
│   └── test_set/                      # Test Data
│       ├── omin_eval.json             # Omin Evaluation Set
│       ├── deepeye_eval.json          # DeepEye Evaluation Set
│       └── qwen_eval.json             # Qwen Evaluation Set
├── src/                               # Source Code
│   ├── data_processing/               # Data Processing Module
│   │   ├── __init__.py
│   │   ├── dpo_train_set_generator.py    # DPO Training Set Generator
│   │   ├── sft_train_set_generator.py    # SFT Training Set Generator
│   │   └── cluster_processor.py          # Clustering Processor
│   ├── training/                         # Training Module
│   │   ├── __init__.py
│   │   ├── SFT.py                        # Full Pipeline Trainer
│   │   └── DPO.py                        # Full Pipeline Trainer
│   ├── test/                             # Testing & Inference Module
│   │   ├── __init__.py
│   │   ├── selector.py                   # Selector Inference
│   │   ├── SC.py                         # SC Baseline Method
│   │   └── evaluation.py                 # Evaluation & Analysis
├── models/                               # Output Directory
│   ├── sft_model/                        # SFT Stage Model
│   └── dpo_model/                        # DPO Stage Model
├── requirements.txt                      # Dependencies List
└── README.md                             # Project Documentation
```

## Installation

```bash
pip install -r requirements.txt
```

Core Dependencies:
- torch >= 2.0.0
- transformers >= 4.30.0
- peft >= 0.4.0
- datasets >= 2.12.0
- sqlparse >= 0.4.4
- scikit-learn >= 1.2.0
- numpy >= 1.21.0
- pandas >= 1.5.0

## Quick Start

### 1. Data Preparation

Download the BIRD development dataset:

- Visit https://bird-bench.github.io/
- Download dev.zip from the website
- Save it to the data/ directory

### 2. Model Training

```bash
# SFT supervised fine-tuning
python main.py --mode train_sft --model_path Qwen/Qwen2.5-3B

# DPO reinforcement learning training
python main.py --mode train_dpo --sft_model_path outputs/models/sft_model
```

### 3. Model Evaluation

```bash
# Evaluate using selector
python selector.py

# Get self-consistency baseline
python SC.py

# Evaluate the differnece between the two methods
python evaluation.py

```

## Dataset Description

### Training Data
- **SFT Data**: Contains (schema, natural_language_question, candidates, correct_sql) quadruples
- **DPO Data**: Contains (schema, natural_language_question, candidates, chosen_sql, rejected_sql) quintuples

### Test Data
- **omin_eval.json**: Evaluation set with candidate SQLs generated by Omin model
- **deepeye_eval.json**: Evaluation set with candidate SQLs generated by DeepEye model  
- **qwen_eval.json**: Evaluation set with candidate SQLs generated by Qwen model


## Experimental Results

| SQL Generation Model | Selection Strategy | Dev Execution Accuracy |
|----------------------|-------------------|------------------------|
| Qwen2.5-Coder-7b-Instruct | SC Baseline | 000% |
| XiyanSQL-7b | Selector | 000% | 

## Configuration

### Training Parameters
- Base Model: Qwen2.5-3B
- LoRA Rank: 16
- Learning Rate: 2e-4 (SFT), 1e-5 (DPO)
- Batch Size: 8
- Epochs: 3 (SFT), 2 (DPO)

### Test Set Generation Parameters
- **Qwen2.5-Coder-7B-Instruct**
- Temperature: 0.8
- Num-return-sequences: 8
- Max Output Length: 1024

- **XiyanSQL-7B**
- Temperature: 0.7
- Num-return-sequences: 8
- Max Output Length: 1024

## Acknowledgments

- Thanks to the Qwen team for providing the base model
- Thanks to the BIRD dataset providers
- Thanks to all contributors and users
