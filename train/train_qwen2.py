# train_qwen_dpo.py
import os
import random
from pathlib import Path
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
)
from peft import (
    LoraConfig,
    prepare_model_for_kbit_training,
)
from trl import DPOTrainer, DPOConfig
import numpy as np

# ----------------------------------------------------------------------
# é…ç½®å‚æ•°
# ----------------------------------------------------------------------
MODEL_NAME_OR_PATH = "Qwen/Qwen2.5-Coder-7B-Instruct"
DATA_FILE_PATH = "train_judge_dpo.jsonl" 
OUTPUT_DIR = "./qwen7b_judge_dpo_adapter"
NUM_TRAIN_EPOCHS = 1

# ----------------------------------------------------------------------
# ä¼˜åŒ–çš„æ•°æ®æ ¼å¼åŒ–å‡½æ•°
# ----------------------------------------------------------------------
def format_dpo_dataset(example: dict) -> dict:
    """ä¼˜åŒ–ç‰ˆæœ¬çš„æ•°æ®æ ¼å¼åŒ–"""
    
    question = example['question']
    chosen_sql = example['chosen']['sql']
    chosen_ast = example['chosen']['ast']
    rejected_sql = example['rejected']['sql']
    rejected_ast = example['rejected']['ast']

    # éšæœºåŒ–é¡ºåº
    if random.random() < 0.5:
        sql_a, ast_a = chosen_sql, chosen_ast
        sql_b, ast_b = rejected_sql, rejected_ast
        chosen_response = "A"
        rejected_response = "B"
    else:
        sql_a, ast_a = rejected_sql, rejected_ast
        sql_b, ast_b = chosen_sql, chosen_ast
        chosen_response = "B"
        rejected_response = "A"
        
    # æ„å»ºprompt - ä½¿ç”¨æ›´ç®€æ´çš„æ¨¡æ¿
    user_prompt = f"""Task: Evaluate two SQL queries given a question. Choose the query (A or B) that correctly answers the question.

Question: {question}

Query [A]: {sql_a}
AST [A]: {ast_a}

Query [B]: {sql_b}  
AST [B]: {ast_b}

Which query is correct (A or B)? Respond with a single letter (A or B) only."""

    final_prompt_string = (
        f"<|im_start|>system\n"
        f"You are an expert SQL analyst. Your task is to judge the correctness of SQL queries.\n<|im_end|>\n"
        f"<|im_start|>user\n{user_prompt}\n<|im_end|>\n"
        f"<|im_start|>assistant\n"
    )

    return {
        "prompt": final_prompt_string,
        "chosen": chosen_response,
        "rejected": rejected_response
    }

# ----------------------------------------------------------------------
# é¢„æ ¼å¼åŒ–æ•°æ®é›†ï¼ˆå‡å°‘è®­ç»ƒæ—¶çš„å¤„ç†å¼€é”€ï¼‰
# ----------------------------------------------------------------------
def preprocess_dataset():
    """é¢„å¤„ç†æ•°æ®é›†å¹¶ä¿å­˜åˆ°ç£ç›˜"""
    processed_data_path = "processed_dpo_dataset"
    
    if os.path.exists(processed_data_path):
        print(f"åŠ è½½å·²é¢„å¤„ç†çš„æ•°æ®é›†: {processed_data_path}")
        return load_dataset("json", data_files=processed_data_path, split="train")
    
    print("é¢„å¤„ç†æ•°æ®é›†...")
    dataset = load_dataset("json", data_files=DATA_FILE_PATH, split="train")
    
    # é¢„å¤„ç†æ‰€æœ‰æ•°æ®
    processed_data = []
    for example in dataset:
        processed_data.append(format_dpo_dataset(example))
    
    # ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®
    import json
    with open(processed_data_path, 'w') as f:
        for item in processed_data:
            f.write(json.dumps(item) + '\n')
    
    print(f"æ•°æ®é›†å·²é¢„å¤„ç†å¹¶ä¿å­˜åˆ°: {processed_data_path}")
    return load_dataset("json", data_files=processed_data_path, split="train")

# ----------------------------------------------------------------------
# ä¸»è®­ç»ƒæµç¨‹
# ----------------------------------------------------------------------
def main():
    # æ£€æŸ¥ç°æœ‰æ£€æŸ¥ç‚¹
    output_path = Path(OUTPUT_DIR)
    resume_from_checkpoint = False
    if output_path.exists():
        checkpoints = list(output_path.glob("checkpoint-*"))
        if checkpoints:
            latest_checkpoint = max(checkpoints, key=os.path.getctime)
            print(f"å°†ä»ä¸­æ–­å¤„ç»§ç»­è®­ç»ƒ: {latest_checkpoint}")
            resume_from_checkpoint = str(latest_checkpoint)

    print("--- 1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®é›† ---")
    dataset = preprocess_dataset()
    print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")

    # æ•°æ®é›†åˆ†ç‰‡ï¼ˆå¤šGPUè®­ç»ƒï¼‰
    if torch.cuda.device_count() > 1:
        dataset = dataset.shard(num_shards=torch.cuda.device_count(), index=0)
        print(f"åˆ†ç‰‡åæ•°æ®é›†å¤§å°: {len(dataset)}")

    print("--- 2. åŠ è½½æ¨¡å‹ ---")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME_OR_PATH,
        dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    model.config.use_cache = False
    # è®©æ¢¯åº¦æ£€æŸ¥ç‚¹æ­£å¸¸å·¥ä½œ
    if hasattr(model, "enable_input_require_grads"):
        model.enable_input_require_grads()
    model.gradient_checkpointing_enable()
    model = prepare_model_for_kbit_training(model)

    print("--- 3. åŠ è½½ Tokenizer ---")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token


    print("--- 4. é…ç½® LoRA ---")
    peft_config = LoraConfig(
        r=32,  # å¢åŠ rankä»¥è·å¾—æ›´å¥½æ€§èƒ½
        lora_alpha=64,
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj",
        ],
    )

    print("--- 5. é…ç½®ä¼˜åŒ–çš„è®­ç»ƒå‚æ•° ---")
    training_args = DPOConfig(
        output_dir=OUTPUT_DIR,
        # æ˜¾è‘—å¢åŠ æ‰¹æ¬¡å¤§å°ä»¥å……åˆ†åˆ©ç”¨A800æ˜¾å­˜
        per_device_train_batch_size=16,  # æ ¹æ®æ‚¨çš„æ•°æ®é•¿åº¦è°ƒæ•´
        per_device_eval_batch_size=8,
        gradient_accumulation_steps=4,  # æ€»æ‰¹æ¬¡å¤§å° = 8 * 8 * 2(GPU) = 128
        
        # å­¦ä¹ ç‡é…ç½®
        learning_rate=1e-4,  # ç¨å¾®æé«˜å­¦ä¹ ç‡
        lr_scheduler_type="cosine",
        warmup_ratio=0.03,
        
        # è®­ç»ƒé…ç½®
        num_train_epochs=NUM_TRAIN_EPOCHS,
        max_steps=-1,
        
        # ä¼˜åŒ–å™¨é…ç½®
        optim="adamw_torch_fused",  # ä½¿ç”¨èåˆä¼˜åŒ–å™¨
        adam_beta1=0.9,
        adam_beta2=0.95,
        weight_decay=0.1,
        fp16=True,
        
        # æ—¥å¿—å’Œä¿å­˜
        logging_steps=10,
        save_strategy="steps",
        save_steps=200,
        save_total_limit=3,
        
        # æ•°æ®åŠ è½½ä¼˜åŒ–
        dataloader_pin_memory=True,
        dataloader_num_workers=4,  # å¢åŠ æ•°æ®åŠ è½½ workers
        dataloader_prefetch_factor=2,
        
        # DPO ç‰¹å®šé…ç½®
        beta=0.1,
        max_prompt_length=1024,  # æ ¹æ®å®é™…æ•°æ®è°ƒæ•´
        max_length=1152,
        
        # æ¢¯åº¦é…ç½®
        max_grad_norm=0.5,
        gradient_checkpointing=True,
        
        report_to="none",
        ddp_find_unused_parameters=False,
    )

    print("--- 6. åˆå§‹åŒ– DPOTrainer ---")
    dpo_trainer = DPOTrainer(
        model=model,
        ref_model=None,
        args=training_args,
        train_dataset=dataset,
        peft_config=peft_config
    )

    print("--- 7. ğŸš€ å¼€å§‹è®­ç»ƒ ---")
    if hasattr(torch, "compile") and os.name != "nt":
        dpo_trainer.model = torch.compile(dpo_trainer.model)
    else:
        print("Skip torch.compile on Windows (MSVC cl not found).")

    if torch.cuda.is_available():
        print(f"CUDA available. devices={torch.cuda.device_count()}, name={torch.cuda.get_device_name(0)}")
    else:
        print("CUDA not available. Training on CPU -> ä½ GPU åˆ©ç”¨ç‡æ˜¯é¢„æœŸã€‚")


    dpo_trainer.train(resume_from_checkpoint=resume_from_checkpoint)

    print("--- 8. ä¿å­˜æ¨¡å‹ ---")
    dpo_trainer.save_model()
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åˆ°: {OUTPUT_DIR}")

if __name__ == "__main__":
    # è®¾ç½®ç¯å¢ƒå˜é‡ä¼˜åŒ–
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
    main()